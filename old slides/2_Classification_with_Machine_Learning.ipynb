{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification with Machine Learning**\n",
        "\n",
        "In this lesson, we learn how to solve a classification problem through Machine Learning classifiers, *i.e.* model that are able to automatically learn how to solve a problem.\n",
        "\n",
        "**It is absolutely recommended to read the documentation relating to the functions and methods used!**\n",
        "Usually, it is sufficient typing on Google the name of the function (and eventually the name of the library used)."
      ],
      "metadata": {
        "id": "wFjJxN88-_Xw"
      },
      "id": "wFjJxN88-_Xw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import some libraries\n",
        "In particular, `sklearn` is the library for the Machine Learning stuff!"
      ],
      "metadata": {
        "id": "FQx0egnB_ZTb"
      },
      "id": "FQx0egnB_ZTb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39427db-e6d8-49d0-a395-6d96344edaba",
      "metadata": {
        "id": "d39427db-e6d8-49d0-a395-6d96344edaba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions and Classes\n",
        "This is the class that we'll use to handle coordinates of the dataset. We assume to work with only 2D $(x,y)$  coordinates."
      ],
      "metadata": {
        "id": "jKOGvTBK_dPb"
      },
      "id": "jKOGvTBK_dPb"
    },
    {
      "cell_type": "code",
      "source": [
        "class Point:\n",
        "    x = None\n",
        "    y = None"
      ],
      "metadata": {
        "id": "QHoO8Wi6QWmv"
      },
      "id": "QHoO8Wi6QWmv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_labels()` is a function that receives a name (`string`) and returns the class (`int`), following this:\n",
        "\n",
        "*   Triangle: 0\n",
        "*   Rectangle: 1\n",
        "*   Square: 2\n",
        "*   Rhombus: 3\n",
        "\n",
        "Example: 0_triangle.png â†’ 0"
      ],
      "metadata": {
        "id": "xI3jL4Kd_ksT"
      },
      "id": "xI3jL4Kd_ksT"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(name):\n",
        "    if 'triangle' in name:\n",
        "        return 0\n",
        "    elif 'square' in name:\n",
        "        return 1\n",
        "    elif 'rectangle' in name:\n",
        "        return 2\n",
        "    elif 'rhombus' in name:\n",
        "        return 3\n",
        "    else:\n",
        "        raise NotImplementedError('Not existing class!')"
      ],
      "metadata": {
        "id": "vJitaZqSPjXR"
      },
      "id": "vJitaZqSPjXR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`prepare__data()` is a function that prepare the data for the computation.\n",
        "Specifically, returns two lists: `coordinates` and `labels`.\n",
        "In this exercise, we exclude `triangles` from classes for simplicity."
      ],
      "metadata": {
        "id": "FGN5pf8jAtJU"
      },
      "id": "FGN5pf8jAtJU"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(lines):\n",
        "    labels = []\n",
        "    coordinates = []\n",
        "\n",
        "    for line in lines:\n",
        "        content = line.split()\n",
        "\n",
        "        # let's exclude triangles\n",
        "        if 'triangle' not in content[0]:\n",
        "            # create label\n",
        "            labels.append(get_labels(content[0]))\n",
        "\n",
        "            # coordinates\n",
        "            coordinates.append([float(x) for x in content[1:]])\n",
        "\n",
        "    return coordinates, labels"
      ],
      "metadata": {
        "id": "NXxyK0sRPmOW"
      },
      "id": "NXxyK0sRPmOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Body of the solution\n",
        "Upload the file `shapes.txt`.\n",
        "Open the dataset file `shapes.txt` and read the content"
      ],
      "metadata": {
        "id": "zM7mgZQxBub7"
      },
      "id": "zM7mgZQxBub7"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file_path = 'shapes.txt'\n",
        "with open(dataset_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print('Read {} lines'.format(len(lines)))"
      ],
      "metadata": {
        "id": "F9UfwAokQcMJ"
      },
      "id": "F9UfwAokQcMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We **shuffle** the data to change the initial order.\n",
        "It is important in order to have a train and a validation set with all classes.\n",
        "\n",
        "**Tools**:\n",
        "-    `np.random.shuffle()`: modify a sequence in-place by shuffling its contents."
      ],
      "metadata": {
        "id": "nL_HkwmsCCQ0"
      },
      "id": "nL_HkwmsCCQ0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before shuffling: {}'.format(lines[:10]))\n",
        "np.random.shuffle(lines)\n",
        "print('Before shuffling: {}'.format(lines[:10]))"
      ],
      "metadata": {
        "id": "SXKiO_paQjI4"
      },
      "id": "SXKiO_paQjI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differently from the previous exercitation, **in this case it is essential to have a training, validation and test sets**.\n",
        "Training data are used to train the model, while the validation split is used to assess performance.\n",
        "\n",
        "Here, we use validation and test set as synonymous, since we do not have a real test set.\n",
        "\n",
        "We put **20% of data in training, 20% in validation**, and the remaining **60% in the test set**."
      ],
      "metadata": {
        "id": "wKpyTz4HCLcC"
      },
      "id": "wKpyTz4HCLcC"
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = lines[:int(0.2*len(lines))]\n",
        "valset = lines[int(0.2*len(lines)):int(0.4*len(lines))]\n",
        "testset = lines[int(0.4*len(lines)):]\n",
        "print('Total: {} splitted in Train: {}, Val: {} and Test: {}'.format(len(lines), len(trainset), len(valset), len(testset)))"
      ],
      "metadata": {
        "id": "fipgi_u-QnlP"
      },
      "id": "fipgi_u-QnlP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also another way to create the train/val/est splits.\n",
        "\n",
        "**Tools**:\n",
        "\n",
        "*   `train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)`: splits arrays or matrices into random train and test subsets. It is also possible to shuffle data.\n",
        "\n"
      ],
      "metadata": {
        "id": "79v-BHLvLvMU"
      },
      "id": "79v-BHLvLvMU"
    },
    {
      "cell_type": "code",
      "source": [
        "# to apply this method we need two different lists: X (data) and y (labels)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "HG_jr9O7LsNp"
      },
      "id": "HG_jr9O7LsNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this moment, we will have three sets: train, validation and test set.\n",
        "\n",
        "A single datapoint belongs only to one, **these three sets are completely disjointed**.\n",
        "\n",
        "It is important to keep them separated!"
      ],
      "metadata": {
        "id": "4x4XwcitCwig"
      },
      "id": "4x4XwcitCwig"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = prepare_data(trainset)\n",
        "val_x, val_y = prepare_data(valset)\n",
        "test_x, test_y = prepare_data(testset)\n",
        "print('Train: {}, Val: {} and Test: {}'.format(len(train_x), len(val_x), len(test_x)))\n",
        "print('Total: {}'.format(len(train_x) + len(val_x) + len(test_x)))"
      ],
      "metadata": {
        "id": "SbKcMlOWQtTn"
      },
      "id": "SbKcMlOWQtTn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier\n",
        "Here, we define what classifier we are going to use to solve our classification problem. Let's use the SVM implementation of the `sklearn` library.\n"
      ],
      "metadata": {
        "id": "u1My56AGDIXj"
      },
      "id": "u1My56AGDIXj"
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.arrayprint import format_float_scientific\n",
        "clf = svm.SVC(gamma=0.001, C=100., kernel='rbf', verbose=False, probability=False)\n",
        "# clf = RandomForestClassifier()\n",
        "# clf = AdaBoostClassifier()\n",
        "# clf = DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "ZF5xpgPfQ36w"
      },
      "id": "ZF5xpgPfQ36w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Now we are ready for the training!\n",
        "With `sklearn` library is tremendously simple, we just need training data (`train_x` and the related labels `train_y`) and pass them to the classifier.\n",
        "\n",
        "**Tools**:\n",
        "-   `model.fit()`: fit the provided model with training data."
      ],
      "metadata": {
        "id": "RLkeVCUxDcIl"
      },
      "id": "RLkeVCUxDcIl"
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "uaoAX6iwQ5hd"
      },
      "id": "uaoAX6iwQ5hd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation\n",
        "\n",
        "It's time to validate the trained model, in order to find proper hyperparameters.\n",
        "\n",
        "**Tools:**\n",
        "*   `score()`: evaluates the quality of a modelâ€™s predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNNuHP0_hHYb"
      },
      "id": "eNNuHP0_hHYb"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Validation accuracy: {:.3f}'.format(clf.score(val_x, val_y)))"
      ],
      "metadata": {
        "id": "SmdAmrAfhKKi"
      },
      "id": "SmdAmrAfhKKi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "Now we are reading to use our classifier! The trained classifier outputs the labels (as defined above) for the classification task.\n",
        "\n",
        "Tools:\n",
        "  - `model.predict()`: predict the class of the given data."
      ],
      "metadata": {
        "id": "Tuyy2J6hE9V8"
      },
      "id": "Tuyy2J6hE9V8"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = clf.predict(test_x)\n",
        "print('Predicted {} samples: {}'.format(len(pred_y), pred_y))\n",
        "print('GT {} samples: {}'.format(len(test_y), test_y))"
      ],
      "metadata": {
        "id": "_hDdyRVIQ99P"
      },
      "id": "_hDdyRVIQ99P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to understand the how good is the trained classifier.\n",
        "\n",
        "**Tools**:\n",
        "   * `accuracy_score()`: accuracy classification score. The set of labels predicted for a sample must exactly match the corresponding set of labels of GT."
      ],
      "metadata": {
        "id": "gCXwtZzSK9H0"
      },
      "id": "gCXwtZzSK9H0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Final Accuracy: {:.3f}'.format(accuracy_score(test_y, pred_y)))"
      ],
      "metadata": {
        "id": "ln0j7RcbQ_9k"
      },
      "id": "ln0j7RcbQ_9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presumably, you have obtained a lower performance w.r.t. the previous exercitation (based on PR), but rememeber that:\n",
        "- Now the model has **automatically learned** how to solve the classification problem;\n",
        "- The classification problem is quite simple, since we know how to classify geometric shapes. Then, we have a good level of a-priori knowledge."
      ],
      "metadata": {
        "id": "Hdut2Imz5y6A"
      },
      "id": "Hdut2Imz5y6A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise/Homework\n",
        "\n",
        "1) Try to obtain the highest accuracy in classification!\n",
        "You can use:\n",
        "- Different **classifiers**:\n",
        "  *   Tree classifiers, Random Forest, AdaBoostClassifier, ...\n",
        "  *   You can also install other packages (for instance, `xgboost`)\n",
        "\n",
        "You can find a list of several classifiers available in scikit-learn  library  here: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
        "Remember to import the classifiers from sklearn package!\n",
        "\n",
        "- Different **data** in input (you can provide not only the raw coordinates of the shapes, but also other values like diagonals and so on).\n",
        "- Different **normalization** of data.\n",
        "- Different **data splits** (you can vary the amount of samples in train, val and test sets).\n",
        "\n",
        "**Data Normalization**\n",
        "The purpose of normalization is to transform data in a way that they have similar distributions. Normalization for instance translates data into the range [0, 1] or [-1, +1] as follows:\n",
        "\n",
        "> `coordinates = (coordinates - np.mean(coordinates)) / np.std(coordinates)`\n",
        "\n",
        "or\n",
        "\n",
        "> `coordinates = (coordinates - np.min(coordinates)) / (np.max(coordinates)-np.min(coordinates))`\n",
        "\n",
        "In our case, in the Euclid dataset all coordinates are already in the range [0, 224] and then normalization is not strictly needed (actually, in some cases decreases the final accuracy, since normalization compresses data within a certain range, reducing the variance).\n",
        "\n",
        "**NB** In order to obtain comparable results, do not shuffle again the dataset. Only modify the `prepare_data()` function, and/or define a new classifier, and then run a new `fit()` and `score()` procedure.\n",
        "\n",
        "2) Obtain prediction probabilities.\n",
        "\n",
        "**Tools:**\n",
        "*   `predict_proba()`: returns the class probabilities for each data point (model must have the parameter `probability` set to `True`!)\n",
        "\n",
        "3) Include also **triangles** in the classification problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "YXeGC4cczmsQ"
      },
      "id": "YXeGC4cczmsQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}