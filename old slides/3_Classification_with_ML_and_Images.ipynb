{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with Machine Learning classifiers and Feature Descriptors\n",
        "\n",
        "In this lesson, we learn how to solve a classification problem through Machine Learning classifiers and two different Feature Descriptors.\n",
        "\n",
        "**It is absolutely recommended to read the documentation relating to the functions and methods used!**\n",
        "Usually, it is sufficient typing on Google the name of the function (and eventually the name of the library used)."
      ],
      "metadata": {
        "id": "B0pAwMwsD9ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start importing some libraries.\n",
        "In particular, `sklearn` is the library for the **Machine Learning stuff**!"
      ],
      "metadata": {
        "id": "tdut7TZxEQm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDBPJpyHebsR"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from os.path import join\n",
        "import cv2\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The seed is important to have **deterministic** experiments."
      ],
      "metadata": {
        "id": "LFF_VUNP4Ebx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1821)"
      ],
      "metadata": {
        "id": "t69ZUKfqel8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions and Classes\n",
        "`get_labels()` is a function that receives a name (`string`) and returns the class (`int`), following this:\n",
        "\n",
        "*   Triangle: 0\n",
        "*   Rectangle: 1\n",
        "*   Square: 2\n",
        "*   Rhombus: 3\n",
        "\n",
        "Example: 0_triangle.png → 0"
      ],
      "metadata": {
        "id": "H2xmaZC9EfLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(name):\n",
        "    if 'triangle' in name:\n",
        "        return 0\n",
        "    elif 'square' in name:\n",
        "        return 1\n",
        "    elif 'rectangle' in name:\n",
        "        return 2\n",
        "    elif 'rhombus' in name:\n",
        "        return 3\n",
        "    else:\n",
        "        raise NotImplementedError('Not existing class!')"
      ],
      "metadata": {
        "id": "0PJ6kKpSeolR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`extract_feature()` is a function that, given a list of images, compute a Feature Descriptor.\n",
        "We are going to use two libraries: `opencv` to handle images (opening, resizing) and `skimage` to compute features.\n",
        "\n",
        "Specifically:\n",
        "* `feat_type=1` → function computes HOG\n",
        "* `feat_type=2` → function computes LBP\n",
        "* `feat_type=3` → function does not computes any feature, but simply unroll the input image\n",
        "\n",
        "**Tools**:\n",
        "*   `cv2.imread()`: open an image (0: gray level, 1: BGR)\n",
        "*   `cv2.resize()`: resize an image\n",
        "*   `hog()`: compute HOG feature\n",
        "*   `lbp()`: compute LBP feature\n",
        "\n",
        "Be aware that the feature computation time (and the final accuracy of the model) are strictly related to the image size!\n"
      ],
      "metadata": {
        "id": "07rdgQ1ZE0eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, feat_type, img_size):\n",
        "\n",
        "    labels = []\n",
        "    features = []\n",
        "\n",
        "    for image in tqdm(images):\n",
        "\n",
        "        img = cv2.imread(image, 0)\n",
        "\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "        if feat_type == 'hog':\n",
        "            feat = hog(img, orientations=8, pixels_per_cell=(4, 4), cells_per_block=(1, 1))\n",
        "        elif feat_type == 'lbp':\n",
        "            feat = np.ravel(local_binary_pattern(img, P=100, R=5))\n",
        "        elif feat_type == 'img':\n",
        "            img = img / 256.0\n",
        "            feat = np.ravel(img)\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented feature!')\n",
        "\n",
        "        features.append(feat)\n",
        "        labels.append(get_labels(image))\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "A_WRkcIHesWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "\n",
        "1.   Upload the `.zip` file containing the *Euclid* dataset\n",
        "2.   Unzip the file using the following comand. Dataset folders will appear in `/content`\n",
        "\n"
      ],
      "metadata": {
        "id": "QFZvmuB8Hz0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Euclid_dataset.zip -d /content"
      ],
      "metadata": {
        "id": "oHeWiBsdfOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a list of all images available in the dataset.\n",
        "\n",
        "**Tools**:\n",
        "* `join()`: joins one or more path components intelligently. The return value is the concatenation of path.\n",
        "* `glob()`: returns a possibly empty list of path names that match names. Wildcards (in that case `*` are allowed!)"
      ],
      "metadata": {
        "id": "imH9SgIjJwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/Euclid_dataset'\n",
        "images = glob(join(dataset_path, '*', '*.png'))\n",
        "print('Images: ', len(images))"
      ],
      "metadata": {
        "id": "9Q8hgYWhfy0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this case it is essential to have a **training**, **validation** and **test** sets.\n",
        "\n",
        "Training data are used to train the model, while the validation split is used to assess performance.\n",
        "\n",
        "Here, we use validation and test set as synonymous, since we do not have a real test set.\n",
        "\n",
        "We put **60% of data in training**, **10% in validation**, and the remaining **30% in the test set**."
      ],
      "metadata": {
        "id": "YzS4_3PyKz3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(images)\n",
        "trainset = images[:int(0.6*len(images))]\n",
        "valset = images[int(0.6*len(images)):int(0.7*len(images))]\n",
        "testset = images[int(0.7*len(images)):]\n",
        "print('Total: {} splitted in Train: {}, Val: {} and Test: {}'.format(len(images), len(trainset), len(valset), len(testset)))"
      ],
      "metadata": {
        "id": "RSfto7GEf3or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define two important elements: the **size of the images** (used to compute feature descriptors) and the **type of features**.\n",
        "We use a **progress bar** (`tqdm`) to show the state of the feature computation!"
      ],
      "metadata": {
        "id": "LNOQaDmiLGr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 112\n",
        "feature_type = 'hog'\n",
        "\n",
        "train_x, train_y = extract_features(trainset, feature_type, img_size)\n",
        "val_x, val_y = extract_features(valset, feature_type, img_size)\n",
        "test_x, test_y = extract_features(testset, feature_type, img_size)"
      ],
      "metadata": {
        "id": "upXl8tRpf6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier\n",
        "Now that the feature extraction is ended, we can define our classifiers.\n",
        "As in the previous case, we start our analysis from the SVM."
      ],
      "metadata": {
        "id": "wGSi5BgULZkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(gamma=0.001, C=100., kernel='rbf', verbose=False)"
      ],
      "metadata": {
        "id": "OjB26pJIgIOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Now we are ready for the training!\n",
        "With `sklearn` library is tremendously simple, we just need training data (`train_x` and the related labels `train_y`) and pass them to the classifier.\n",
        "\n",
        "**Tools**:\n",
        "-   `model.fit()`: fit the provided model with training data."
      ],
      "metadata": {
        "id": "l-ythDtFLx3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "QfKancpNgJDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "uxviII1Hlffv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(val_x, val_y)"
      ],
      "metadata": {
        "id": "j25OnUKUlg9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "Now we are reading to use our classifier! The trained classifier output the labels (as defined above) for the classification task.\n",
        "\n",
        "Tools:\n",
        "  - `model.predict()`: predict the class of the given data."
      ],
      "metadata": {
        "id": "M4HUxo3sL1Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(test_x)\n",
        "print('Predicted {} samples: {}'.format(len(y_pred), y_pred))"
      ],
      "metadata": {
        "id": "EcMai76lgLM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to understand the final performance of the trained classifier.\n",
        "\n",
        "**Tools**:\n",
        "   * `accuracy_score()`: Accuracy classification score. The set of labels predicted for a sample must exactly match the corresponding set of labels of GT."
      ],
      "metadata": {
        "id": "x8mJc9E4MBnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Final Accuracy: {:.3f}'.format(accuracy_score(test_y, y_pred)))"
      ],
      "metadata": {
        "id": "B_ARZOB5gMqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix\n",
        "\n",
        "We can also compute the confusion matrix to further understand the performance on the trained model.\n",
        "\n",
        "**Tools**:\n",
        "   * `confusion_matrix()`: computes confusion matrix to evaluate the accuracy of a classification.\n",
        "   * `plot_confusion_matrix()`: plots Confusion Matrix (it is deprectaed and will be removed in future versions of the library)."
      ],
      "metadata": {
        "id": "wAbkr3SsOoka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = confusion_matrix(test_y, y_pred)\n",
        "print(matrix)\n",
        "print(matrix.diagonal() / matrix.sum(axis=1))\n",
        "\n",
        "cm = ConfusionMatrixDisplay(matrix)\n",
        "cm.plot()"
      ],
      "metadata": {
        "id": "6zdRkhyDgkS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Try to use different **classifiers**.\n",
        "In addition, you can try to change the parameters used to resize images and compute features:\n",
        "- What changes can you note in **computational load**?\n",
        "- What changes can you note in **performance**?\n",
        "\n",
        "What further considerations can you observe thanks to the confusion matrix?\n"
      ],
      "metadata": {
        "id": "2gB4pNMnxlMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "1) Write your own function to compute the **confusion matrix** and the **diagonal** with the classification scores for each class."
      ],
      "metadata": {
        "id": "DwDiKvLAxazj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_confusion_matrix(test_y, pred_y):\n",
        "\n",
        "  # compute the confusion matrix\n",
        "\n",
        "  # compute the diagonal\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "uLSFWqdWxgDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_confusion_matrix(test_y, pred_y)"
      ],
      "metadata": {
        "id": "iEInhG9wxgkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Download the CIFAR 10 dataset from Virtuale (`CIFAR-10-simple.zip`).\n",
        "Load the dataset and the classes, try to solve the classification problem. You can use only the data provided in the `train` folder to train your model.\n",
        "Compute the final accuracy with the folder `test`.\n",
        "\n",
        "CIFAR 10 (https://www.cs.toronto.edu/~kriz/cifar.html) consists in **32x32 colour images** (RGB) divided in **10 classes**. There are **300 samples in training** and **50 testing samples** for each class.\n"
      ],
      "metadata": {
        "id": "CeMWDQRs4Yxe"
      }
    }
  ]
}