{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with Deep Learning\n",
        "\n",
        "In this lesson, we learn how to solve a classification problem through a **Deep Learning** approach (*Multi Layer Perceptron*, MLP).\n",
        "\n",
        "**It is absolutely recommended to read the documentation related to the functions and methods used!**\n",
        "Usually, it is sufficient typing on Google the name of the function (and eventually the name of the library used)."
      ],
      "metadata": {
        "id": "qg7KL0aWQwJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import some libraries. We will use [*TensorFlow*](https://www.tensorflow.org/) and [*Keras*](https://keras.io/) as Deep Learning frameworks."
      ],
      "metadata": {
        "id": "ZssyrYnDRGmR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcH4Xacbg7bF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "\n",
        "1.   Upload the `.zip` file containing the *Euclid* dataset\n",
        "2.   Unzip the file using the following comand. Dataset folders will appear in `/content`"
      ],
      "metadata": {
        "id": "SLXpysTPRVJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Euclid_dataset.zip -d /content"
      ],
      "metadata": {
        "id": "z6gGq9EshL37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders\n",
        "\n",
        "We can use already developed methods to load data for our DL solution.\n",
        "We split the dataset with **80% of data** in the **training** and the remaining **20% in the validation** set.\n",
        "\n",
        "**Tools**:\n",
        "   * `image_dataset_from_directory()`: generates a dataset from image files in a directory, that yields batches of images from the subdirectories present in our directory."
      ],
      "metadata": {
        "id": "GhllMIO4Rb3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train** dataset"
      ],
      "metadata": {
        "id": "k4b3sZ1ASan6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='Euclid_dataset',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=['triangle', 'rectangle', 'square', 'rhombus'],\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    image_size=(32, 32),\n",
        "    shuffle=True,\n",
        "    seed=1821,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        ")"
      ],
      "metadata": {
        "id": "goHMzLR-hBPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation** dataset"
      ],
      "metadata": {
        "id": "2xzZ26miSd_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='Euclid_dataset',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=['triangle', 'rectangle', 'square', 'rhombus'],\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    image_size=(32, 32),\n",
        "    shuffle=True,\n",
        "    seed=1821,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Bo-hHneVhDbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architecture\n",
        "It's time to define the architecture of our *Multi Layer Perceptron* (MLP).\n",
        "We create:\n",
        "*   1 **input** layer with 128 neurons\n",
        "*   1 **output** layer with 4 (our classes!) neurons\n",
        "*   2 **hidden layers** with 64 ans 32 neurons, respectively.\n",
        "\n",
        "\n",
        "\n",
        "**Tools**:\n",
        "*   `Normalization()`: normalize input data.\n",
        "*   `Flatten()`: flattens (2D â†’ 1D) the input.\n",
        "*   `Dense()`: just your regular densely-connected NN layer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lDJaJ9lSSqtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our own architecture\n",
        "model=keras.models.Sequential([\n",
        "    tf.keras.layers.Normalization(axis=-1, mean=None, variance=None),\n",
        "    keras.layers.Flatten(input_shape=(32, 32,)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "ImPF3vGHhm58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define:\n",
        "\n",
        "*   number of **epochs**\n",
        "*   model saving\n",
        "*   **optimizer**\n",
        "*   **loss** function\n",
        "\n",
        "We also define the **callbacks**: a callback is an object (method) that can perform actions at various stages of training (*e.g.* at the start or end of an epoch, before or after a single batch, etc).\n",
        "\n",
        "We need to **compile** the model before the training phase."
      ],
      "metadata": {
        "id": "BaOFM9KxbTop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "\n",
        "callbacks = [\n",
        "    # to save the model after every epoch\n",
        "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n",
        "    # logging\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\", write_graph=True, write_images=False, update_freq=\"epoch\",)\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "metadata": {
        "id": "hD_8QfGEhvfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "The command to train the model is the same of the Scikit-Learn package!"
      ],
      "metadata": {
        "id": "BargpIWIbsft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n",
        ")"
      ],
      "metadata": {
        "id": "Bt6J16Xbhzir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "The final accuracy score is negatively affected by:\n",
        "*   The **lack** of **training** data.\n",
        "*   The use of MLP with high-dimensional data (unrolled images) usually is not the best solution. Probably, it's better to extract manual features (such as HOG and LBP). We'll see how to use *Convolutiona Neural Networks* to tackle this problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "pUcW0P6M6pj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations\n",
        "\n",
        "With the command `summary()` we can check the architecture of our Neural Network."
      ],
      "metadata": {
        "id": "uGFcggSj2h-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "DdGf5op52cQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load the **TensorBoard** module (provided with Tensorflow) to see our line plots!"
      ],
      "metadata": {
        "id": "zpE7HsHn5Sb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "KnEO393A3vGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "Use the MLP classifier with the Scikit-Learn library (use the code of the lab exercitation 2 or 3). In particular, use **coordinates** for input (not images)."
      ],
      "metadata": {
        "id": "84wdNbCUHE5y"
      }
    }
  ]
}