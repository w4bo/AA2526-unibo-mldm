{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression with Machine Learning**\n",
        "\n",
        "In this lesson, we learn how to solve a regression problem through Machine Learning regressors.\n",
        "\n",
        "**It is absolutely recommended to read the documentation relating to the functions and methods used!**\n",
        "Usually, it is sufficient typing on Google the name of the function (and eventually the name of the library used)."
      ],
      "metadata": {
        "id": "wFjJxN88-_Xw"
      },
      "id": "wFjJxN88-_Xw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import some libraries\n",
        "In particular, `sklearn` is the library for the Machine Learning stuff!"
      ],
      "metadata": {
        "id": "FQx0egnB_ZTb"
      },
      "id": "FQx0egnB_ZTb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39427db-e6d8-49d0-a395-6d96344edaba",
      "metadata": {
        "id": "d39427db-e6d8-49d0-a395-6d96344edaba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions and Classes\n",
        "This is the class that we'll use to handle coordinates of the dataset. We assume to work with only 2D $(x,y)$  coordinates."
      ],
      "metadata": {
        "id": "jKOGvTBK_dPb"
      },
      "id": "jKOGvTBK_dPb"
    },
    {
      "cell_type": "code",
      "source": [
        "class Point:\n",
        "    x = None\n",
        "    y = None"
      ],
      "metadata": {
        "id": "QHoO8Wi6QWmv"
      },
      "id": "QHoO8Wi6QWmv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(coordinates):\n",
        "\n",
        "  c = [float(x) for x in coordinates]\n",
        "\n",
        "  if len(coordinates) == 6:\n",
        "        centroid_x = (c[0] + c[2] + c[4]) / 3\n",
        "        centroid_y = (c[1] + c[3] + c[5]) / 3\n",
        "  else:\n",
        "        centroid_x = (c[0] + c[2] + c[4] + c[6]) / 4\n",
        "        centroid_y = (c[1] + c[3] + c[5] + c[7]) / 4\n",
        "\n",
        "  return [centroid_x, centroid_y]"
      ],
      "metadata": {
        "id": "khXmAALYorbj"
      },
      "id": "khXmAALYorbj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`prepare__data()` is a function that prepare the data for the computation.\n",
        "Specifically, returns two lists: `coordinates` and `labels`.\n",
        "In this exercise, we exclude `triangles` from classes for simplicity."
      ],
      "metadata": {
        "id": "FGN5pf8jAtJU"
      },
      "id": "FGN5pf8jAtJU"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(lines):\n",
        "    labels = []\n",
        "    coordinates = []\n",
        "\n",
        "    for line in lines:\n",
        "        content = line.split()\n",
        "\n",
        "        # let's exclude triangles\n",
        "        if 'triangle' not in content[0]:\n",
        "            # create label\n",
        "            labels.append(get_labels(content[1:]))\n",
        "\n",
        "            # coordinates\n",
        "            coordinates.append([float(x) for x in content[1:]])\n",
        "\n",
        "    return coordinates, labels"
      ],
      "metadata": {
        "id": "NXxyK0sRPmOW"
      },
      "id": "NXxyK0sRPmOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Body of the solution\n",
        "Upload the file `shapes.txt`.\n",
        "Open the dataset file `shapes.txt` and read the content"
      ],
      "metadata": {
        "id": "zM7mgZQxBub7"
      },
      "id": "zM7mgZQxBub7"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file_path = 'shapes.txt'\n",
        "with open(dataset_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print('Read {} lines'.format(len(lines)))"
      ],
      "metadata": {
        "id": "F9UfwAokQcMJ"
      },
      "id": "F9UfwAokQcMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We **shuffle** the data to change the initial order.\n",
        "It is important in order to have a train and a validation set with all possible ground truth values.\n",
        "\n",
        "**Tools**:\n",
        "-    `np.random.shuffle()`: modify a sequence in-place by shuffling its contents."
      ],
      "metadata": {
        "id": "nL_HkwmsCCQ0"
      },
      "id": "nL_HkwmsCCQ0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before shuffling: {}'.format(lines[:10]))\n",
        "np.random.shuffle(lines)\n",
        "print('Before shuffling: {}'.format(lines[:10]))"
      ],
      "metadata": {
        "id": "SXKiO_paQjI4"
      },
      "id": "SXKiO_paQjI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It is essential to have a training, validation and test sets**.\n",
        "Training data are used to train the model, while the validation split is used to assess performance.\n",
        "\n",
        "Here, we use validation and test set as synonymous, since we do not have a real test set.\n",
        "\n",
        "We put **20% of data in training, 20% in validation**, and the remaining **60% in the test set**."
      ],
      "metadata": {
        "id": "wKpyTz4HCLcC"
      },
      "id": "wKpyTz4HCLcC"
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = lines[:int(0.6*len(lines))]\n",
        "valset = lines[int(0.6*len(lines)):int(0.8*len(lines))]\n",
        "testset = lines[int(0.8*len(lines)):]\n",
        "print('Total: {} splitted in Train: {}, Val: {} and Test: {}'.format(len(lines), len(trainset), len(valset), len(testset)))"
      ],
      "metadata": {
        "id": "fipgi_u-QnlP"
      },
      "id": "fipgi_u-QnlP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also another way to create the train/val/test splits.\n",
        "\n",
        "**Tools**:\n",
        "\n",
        "*   `train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)`: splits arrays or matrices into random train and test subsets. It is also possible to shuffle data.\n",
        "\n"
      ],
      "metadata": {
        "id": "79v-BHLvLvMU"
      },
      "id": "79v-BHLvLvMU"
    },
    {
      "cell_type": "code",
      "source": [
        "# to apply this method we need two different lists: X (data) and y (labels)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "HG_jr9O7LsNp"
      },
      "id": "HG_jr9O7LsNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this moment, we will have three sets: train, validation and test set.\n",
        "\n",
        "A single datapoint belongs only to one, **these three sets are completely disjointed**.\n",
        "\n",
        "It is important to keep them separated!"
      ],
      "metadata": {
        "id": "4x4XwcitCwig"
      },
      "id": "4x4XwcitCwig"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = prepare_data(trainset)\n",
        "val_x, val_y = prepare_data(valset)\n",
        "test_x, test_y = prepare_data(testset)\n",
        "print('Train: {}, Val: {} and Test: {}'.format(len(train_x), len(val_x), len(test_x)))\n",
        "print('Total: {}'.format(len(train_x) + len(val_x) + len(test_x)))"
      ],
      "metadata": {
        "id": "SbKcMlOWQtTn"
      },
      "id": "SbKcMlOWQtTn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressor\n",
        "Here, we define what regressor we are going to use to solve our regression problem. Let's use the SVR (SVM) implementation of the `sklearn` library.\n"
      ],
      "metadata": {
        "id": "u1My56AGDIXj"
      },
      "id": "u1My56AGDIXj"
    },
    {
      "cell_type": "code",
      "source": [
        "#from numpy.core.arrayprint import format_float_scientific\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "clf = svm.SVR()\n",
        "clf = MultiOutputRegressor(clf)\n",
        "\n",
        "# from sklearn.neural_network import MLPRegressor\n",
        "# clf = MLPRegressor()\n",
        "# clf =\n",
        "# clf ="
      ],
      "metadata": {
        "id": "ZF5xpgPfQ36w"
      },
      "id": "ZF5xpgPfQ36w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Now we are ready for the training!\n",
        "With `sklearn` library is tremendously simple, we just need training data (`train_x` and the related labels `train_y`) and pass them to the regressor.\n",
        "\n",
        "**Tools**:\n",
        "-   `model.fit()`: fit the provided model with training data."
      ],
      "metadata": {
        "id": "RLkeVCUxDcIl"
      },
      "id": "RLkeVCUxDcIl"
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "uaoAX6iwQ5hd"
      },
      "id": "uaoAX6iwQ5hd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation\n",
        "\n",
        "It's time to validate the trained model, in order to find proper hyperparameters.\n",
        "\n",
        "**Tools:**\n",
        "*   `score()`: evaluates the quality of a modelâ€™s predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNNuHP0_hHYb"
      },
      "id": "eNNuHP0_hHYb"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Validation accuracy: {:.3f}'.format(clf.score(val_x, val_y)))"
      ],
      "metadata": {
        "id": "SmdAmrAfhKKi"
      },
      "id": "SmdAmrAfhKKi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "Now we are reading to use our regressor! The trained regressor outputs the labels (as defined above) for the regression task.\n",
        "\n",
        "Tools:\n",
        "  - `model.predict()`: predict the values."
      ],
      "metadata": {
        "id": "Tuyy2J6hE9V8"
      },
      "id": "Tuyy2J6hE9V8"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = clf.predict(test_x)\n",
        "print('Predicted {} samples: {}'.format(len(pred_y), pred_y[:5]))\n",
        "print('GT {} samples: {}'.format(len(test_y), test_y[:5]))"
      ],
      "metadata": {
        "id": "_hDdyRVIQ99P"
      },
      "id": "_hDdyRVIQ99P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for t, p, gt in zip(test_x[:4], pred_y, test_y):\n",
        "\n",
        "  white = np.ones((800, 800, 3)) * 255\n",
        "\n",
        "  cv2.circle(white, (int(t[0]), int(t[1])), 3, (255, 0, 0), -1)\n",
        "  cv2.circle(white, (int(t[2]), int(t[3])), 3, (255, 0, 0), -1)\n",
        "  cv2.circle(white, (int(t[4]), int(t[5])), 3, (255, 0, 0), -1)\n",
        "  cv2.circle(white, (int(t[6]), int(t[7])), 3, (255, 0, 0), -1)\n",
        "\n",
        "  # prediction\n",
        "  print(p)\n",
        "  cv2.circle(white, (int(p[0]), int(p[1])), 3, (0, 0, 255))\n",
        "\n",
        "  # ground truth\n",
        "  print(gt)\n",
        "  cv2.circle(white, (int(gt[0]), int(gt[1])), 3, (0, 255, 0))\n",
        "\n",
        "  cv2_imshow(white)"
      ],
      "metadata": {
        "id": "2rxQFTd2uQ6s"
      },
      "id": "2rxQFTd2uQ6s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to understand the how good is the trained regressor.\n",
        "We need some metrics for regression!"
      ],
      "metadata": {
        "id": "gCXwtZzSK9H0"
      },
      "id": "gCXwtZzSK9H0"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "mae = mean_absolute_error(test_y, pred_y, multioutput='raw_values')\n",
        "print('MAE:', mae)\n",
        "\n",
        "mse = mean_squared_error(test_y, pred_y, multioutput='raw_values')\n",
        "print('MSE:', mse)"
      ],
      "metadata": {
        "id": "0WoHhkDFtExy"
      },
      "id": "0WoHhkDFtExy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise/Homework\n",
        "1) Implement your own MAE, MSE, ...\n",
        "\n",
        "2) Try to obtain the highest accuracy in regression!\n",
        "You can use:\n",
        "- Different **Regressors**:\n",
        "  *   SVR, MLP, ...\n",
        "  *   You can also install other packages (for instance, `xgboost`)\n",
        "\n",
        "Remember to import the regressors from sklearn package!\n",
        "\n",
        "- Different **data** in input (you can provide not only the raw coordinates of the shapes, but also other values like diagonals and so on).\n",
        "- Different **normalization** of data.\n",
        "- Different **data splits** (you can vary the amount of samples in train, val and test sets).\n",
        "\n",
        "**Data Normalization**\n",
        "The purpose of normalization is to transform data in a way that they have similar distributions. Normalization for instance translates data into the range [0, 1] or [-1, +1] as follows:\n",
        "\n",
        "> `coordinates = (coordinates - np.mean(coordinates)) / np.std(coordinates)`\n",
        "\n",
        "or\n",
        "\n",
        "> `coordinates = (coordinates - np.min(coordinates)) / (np.max(coordinates)-np.min(coordinates))`\n",
        "\n",
        "In our case, in the Euclid dataset all coordinates are already in the range [0, 224] and then normalization is not strictly needed (actually, in some cases decreases the final accuracy, since normalization compresses data within a certain range, reducing the variance).\n",
        "\n",
        "**NB** In order to obtain comparable results, do not shuffle again the dataset. Only modify the `prepare_data()` function, and/or define a new regressor, and then run a new `fit()` and `score()` procedure.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YXeGC4cczmsQ"
      },
      "id": "YXeGC4cczmsQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}